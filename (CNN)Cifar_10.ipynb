{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(CNN)Cifar-10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4inEgkanZUZ/2nuY1snCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Knowsoeun/AI_Study/blob/main/(CNN)Cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DApeuNYa-ax"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#Train a simple deep CNN on the CIFAR10 small images dataset.\n",
        "\n",
        "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
        "(it's still underfitting at that point, though).\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import os\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "metadata": {
        "id": "i9OVaGCYbJAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "metadata": {
        "id": "arsRhuI4bbOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))  # layer1 : 32 32x32 Feature Maps이 출력된다.\n",
        "\"\"\"\n",
        "padding='same' : 입력과 출력이 가로, 세로 크기가 같도록 padding을 적절하게 넣어주라는 의미.\n",
        "same이므로 그대로 두라는 의미.\n",
        "\"\"\"\n",
        "model.add(Activation('relu')) # layer2 : 활성화 함수 relu\n",
        "model.add(Conv2D(32, (3, 3))) # layer3 : 32 32 30x30 Feature Maps이 출력된다.\n",
        "model.add(Activation('relu')) # layer4 : 활성화 함수 relu\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # layer5 : 가로, 세로 길이가 반씩 줄어 32 15x15 Feature Maps이 출력된다. \n",
        "model.add(Dropout(0.25)) # layer6 : 학습을 용이하게 하기 위해 넣는 층\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same')) # layer8 : 64 15x15 Feature Maps이 출력된다.\n",
        "model.add(Activation('relu')) # layer9 : 활성화 함수 relu\n",
        "model.add(Conv2D(64, (3, 3))) # layer10 : 64 13x13 Feature Maps이 출력된다.\n",
        "model.add(Activation('relu')) # layer11 : 활성화 함수 relu\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # layer12 : 가로, 세로 길이가 반씩 줄어 64 6x6 Feature Maps이 출력된다.\n",
        "model.add(Dropout(0.25)) # layer13 : 학습을 용이하게 하기 위해 넣는 층\n",
        "\n",
        "model.add(Flatten()) # layer14 : 합성곱층3차원 -> 완전연결층1차원이므로 Flatten(3차원을 1차원으로 구조 변경)을 쓴다.\n",
        "model.add(Dense(512)) # layer15 : 완전연결층. (출력 512개. 활성화 함수 relu)\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5)) # layer17 : 학습을 용이하게 하기 위해 넣는 층\n",
        "model.add(Dense(num_classes)) # layer18 : 완전연결층. (클래스의 개수만큼 출력)\n",
        "model.add(Activation('softmax')) # layer19 : 활성화 함수 softmax\n",
        "\n",
        "# summarize model.\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "88HXhYeKbJUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test),\n",
        "          shuffle=True)"
      ],
      "metadata": {
        "id": "J04907RVbJVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "metadata": {
        "id": "Z4FuqcEvbJZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "metadata": {
        "id": "e3RbApRrbJpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}